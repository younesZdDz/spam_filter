{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dIrHCzDlZLi"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLQfPuqZlh3s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20021010_easy_ham.tar.bz2\"\n",
    "HARD_HAM_URL = DOWNLOAD_ROOT + \"20021010_hard_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"easy_ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL), (\"hard_ham.tar.bz2\", HARD_HAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXQbnYI4lkrV"
   },
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BNzdsZQlnMz"
   },
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "HARD_HAM_DIR = os.path.join(SPAM_PATH, \"hard_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "hard_ham_filenames = [name for name in sorted(os.listdir(HARD_HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ppMfmYdblpEd",
    "outputId": "2252aba5-d24c-4fbb-f74e-ea01c8a3c184"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2551, 250, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames), len(hard_ham_filenames), len(spam_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u58xu3tZlrHZ"
   },
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mVrf1cBls2Q"
   },
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    for part in email.walk():\n",
    "        ctype= part.get_content_type()\n",
    "        if not ctype in ('text/plain', 'text/html'):\n",
    "            continue \n",
    "        try:\n",
    "            content= part.get_content()\n",
    "        except:\n",
    "            content = str(part.get_payload())\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "ham_emails = [email for email in ham_emails if TextBlob(email_to_text(email) or 'bonjour').detect_language()=='en']\n",
    "spam_emails = [email for email in spam_emails if TextBlob(email_to_text(email) or 'bonjour').detect_language()=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2540, 480)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_emails), len(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qna0WGrlyow"
   },
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return 'text/plain'\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ])\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_Yo5m-bl0eO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2553),\n",
       " ('application/pgp-signature', 73),\n",
       " ('text/html', 8),\n",
       " ('application/octet-stream', 2),\n",
       " ('application/x-pkcs7-signature', 2),\n",
       " ('text/enriched', 1),\n",
       " ('application/ms-tnef', 1),\n",
       " ('video/mng', 1),\n",
       " ('text/rfc822-headers', 1),\n",
       " ('application/x-java-applet', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "structures = [get_email_structure(email) for  email in ham_emails]\n",
    "Counter(sum([ structure.split(\", \") for structure in structures], [])).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = structures + [get_email_structure(email) for  email in spam_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [email_to_text(email) or '' for email in ham_emails]\n",
    "contents = contents + [email_to_text(email) or '' for email in spam_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [email['Subject'] for email in ham_emails]\n",
    "subjects = subjects + [email['Subject'] for email in spam_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2d4otpe5l-Vw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "      <th>structure</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Re: New Sequences Window</td>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -05...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[zzzzteana] RE: Alexander</td>\n",
       "      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[zzzzteana] Moscow bomber</td>\n",
       "      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die</td>\n",
       "      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Re: Insert signature</td>\n",
       "      <td>On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 subject  ...   target\n",
       "0               Re: New Sequences Window  ...        0\n",
       "1              [zzzzteana] RE: Alexander  ...        0\n",
       "2              [zzzzteana] Moscow bomber  ...        0\n",
       "3  [IRR] Klez: The Virus That  Won't Die  ...        0\n",
       "4                   Re: Insert signature  ...        0\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\"subject\" : subjects, \"content\" : contents, \"structure\": structures, \n",
    "                     'target' : np.array([0] * len(ham_emails) + [1] * len(spam_emails))})\n",
    "data.drop_duplicates(['subject', 'content'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urlextract\n",
      "  Downloading https://files.pythonhosted.org/packages/47/13/d8c5970ba73b0266cb13c6883f9e7cf37b044e52255208ceb32b0d09594a/urlextract-0.10-py3-none-any.whl\n",
      "Collecting uritools (from urlextract)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/5d/ef3cd3c40b4b97f0cb50cee8e4c5a8a4abc30953e1c7ce7e0d25cb2534c3/uritools-2.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.6/site-packages (from urlextract) (2.6)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.6/site-packages (from urlextract) (1.4.3)\n",
      "Installing collected packages: uritools, urlextract\n",
      "Successfully installed uritools-2.2.0 urlextract-0.10\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XDL9kEVl_4J"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from html import unescape\n",
    "import urlextract \n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Class for carrying all the text pre-processing stuff throughout the project\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.stopwords = stopwords.words('english')\n",
    "\n",
    "        #self.ps = PorterStemmer()  \n",
    "        self.lm = WordNetLemmatizer()\n",
    "        # stemmer will be used for each unique word once\n",
    "        #self.stemmed = dict()\n",
    "        self.lemmetized = dict()\n",
    "\n",
    "        self.url_extractor = urlextract.URLExtract()\n",
    "        \n",
    "\n",
    "    \n",
    "    def process(self, text, allow_stopwords = False, use_stemmer = True) :\n",
    "        \"\"\"\n",
    "        Process the specified text,\n",
    "        splitting by non-alphabetic symbols, casting to lower case,\n",
    "        removing stopwords, HTML tags and stemming each word\n",
    "\n",
    "        :param text: text to precess\n",
    "        :param allow_stopwords: whether to remove stopwords\n",
    "        :return: processed text\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "\n",
    "        # split and cast to lower case\n",
    "        #text = re.sub(r'<[^>]+>', ' ', str(text))        \n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[0-9]+(?:\\.[0-9]+){3}', ' URL ', text)\n",
    "        urls = list(set(self.url_extractor.find_urls(text)))\n",
    "        urls.sort(key=lambda url: len(url), reverse=True)\n",
    "        for url in urls:\n",
    "            text = text.replace(url, \" URL \")\n",
    "            \n",
    "        text = re.sub('<head.*?>.*?</head>', '', text, flags=re.M | re.S | re.I)\n",
    "        text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "        text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "        text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "        text = unescape(text)\n",
    "        text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "       \n",
    "        \n",
    "        text= re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)    \n",
    "        \n",
    "        for word in text.split():\n",
    "            # remove non-alphabetic and stop words\n",
    "            if (word.isalpha() and word not in self.stopwords) or allow_stopwords:\n",
    "                if use_stemmer:\n",
    "                    if word not in self.lemmetized:\n",
    "                        self.lemmetized[word] = self.lm.lemmatize(word)\n",
    "                    # use stemmed version of word\n",
    "                    ret.append(self.lemmetized[word])\n",
    "                else: \n",
    "                    ret.append(word)\n",
    "        return ' '.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "      <th>structure</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Re: New Sequences Window</td>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -05...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[zzzzteana] RE: Alexander</td>\n",
       "      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[zzzzteana] Moscow bomber</td>\n",
       "      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die</td>\n",
       "      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Re: Insert signature</td>\n",
       "      <td>On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 subject  ...   target\n",
       "0               Re: New Sequences Window  ...        0\n",
       "1              [zzzzteana] RE: Alexander  ...        0\n",
       "2              [zzzzteana] Moscow bomber  ...        0\n",
       "3  [IRR] Klez: The Virus That  Won't Die  ...        0\n",
       "4                   Re: Insert signature  ...        0\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "      <th>structure</th>\n",
       "      <th>target</th>\n",
       "      <th>whole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>re new sequence window</td>\n",
       "      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMBE...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>re new sequence window date wed NUMBER aug NUM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzzzteana re alexander</td>\n",
       "      <td>martin posted tasso papadopoulos greek sculpto...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>zzzzteana re alexander martin posted tasso pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzzzteana moscow bomber</td>\n",
       "      <td>man threatens explosion moscow thursday august...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>zzzzteana moscow bomber man threatens explosio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>irr klez the virus that won t die</td>\n",
       "      <td>klez virus die already prolific virus ever kle...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>irr klez the virus that won t die klez virus d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>re insert signature</td>\n",
       "      <td>wed aug NUMBER NUMBER NUMBER NUMBER ulises pon...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>re insert signature wed aug NUMBER NUMBER NUMB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             subject                        ...                                                                      whole\n",
       "0             re new sequence window                        ...                          re new sequence window date wed NUMBER aug NUM...\n",
       "1             zzzzteana re alexander                        ...                          zzzzteana re alexander martin posted tasso pap...\n",
       "2            zzzzteana moscow bomber                        ...                          zzzzteana moscow bomber man threatens explosio...\n",
       "3  irr klez the virus that won t die                        ...                          irr klez the virus that won t die klez virus d...\n",
       "4                re insert signature                        ...                          re insert signature wed aug NUMBER NUMBER NUMB...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = TextProcessor()\n",
    "data_processed = data.copy()\n",
    "data_processed.content = data_processed.content.apply(lambda x: tp.process(x, allow_stopwords=False, use_stemmer=True))\n",
    "data_processed.subject = data_processed.subject.apply(lambda x : tp.process(x, allow_stopwords = True, use_stemmer=True))\n",
    "data_processed['whole']  = data_processed.subject + ' ' + data_processed.content\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc0PnHOPmVuY"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000, column = 'whole'):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.column = column\n",
    "    def fit(self, X, y=None):\n",
    "        counter = []\n",
    "        for text in X[self.column].values:\n",
    "            counter.append(Counter(text.split()))\n",
    "        total_count = Counter()\n",
    "        for word_count in counter:\n",
    "            if isinstance(word_count, list):\n",
    "                print(word_count)\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        counter = []\n",
    "        for text in X[self.column].values:\n",
    "            counter.append(Counter(text.split()))\n",
    "            \n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []        \n",
    "        for row, word_count in enumerate(counter):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)        \n",
    "        return pd.DataFrame(columns=['word_UNK'] + ['word_'+column for column in self.vocabulary_], \n",
    "                            data=csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class StructureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column = 'structure'):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, X, y=None):     \n",
    "        tmp = []\n",
    "        for email in X[self.column].apply(lambda x : x.split(', ')).values :\n",
    "            for structure in email:\n",
    "                tmp.append(structure)        \n",
    "        self.structures = list(Counter(tmp).keys())        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        out = np.zeros((len(X), len(self.structures)))\n",
    "        for i , structure in enumerate(self.structures):\n",
    "            out[:,i] = X[self.column].apply(lambda x : 1 if structure in x.split(', ') else 0).values\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "class LdaTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dim = 2, column = 'whole'):\n",
    "        self.dim = dim\n",
    "        self.column = column\n",
    "    def fit(self, X, y=None):     \n",
    "        lda_tokens = X[self.column].apply(lambda x: x.split())\n",
    "        # create Dictionary and train it on text corpus\n",
    "        self.lda_dic = Dictionary(lda_tokens)\n",
    "        self.lda_dic.filter_extremes(no_below=10, no_above=0.6, keep_n=8000)\n",
    "        lda_corpus = [self.lda_dic.doc2bow(doc) for doc in lda_tokens]\n",
    "        # create TfidfModel and train it on text corpus\n",
    "        self.lda_tfidf = TfidfModel(lda_corpus)\n",
    "        lda_corpus = self.lda_tfidf[lda_corpus]\n",
    "        # create LDA Model and train it on text corpus\n",
    "        self.lda_model = LdaMulticore(\n",
    "            lda_corpus, num_topics=self.dim, id2word=self.lda_dic, workers=4,\n",
    "            passes=20, chunksize=1000, random_state=0\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        lda_emb_len = len(self.lda_model[[]])\n",
    "        lda_corpus = [self.lda_dic.doc2bow(doc) for doc in X[self.column].apply(lambda x: x.split())]\n",
    "        lda_corpus = self.lda_tfidf[lda_corpus]\n",
    "        lda_que_embs = self.lda_model.inference(lda_corpus)[0]\n",
    "        # append lda question embeddings\n",
    "        out = np.zeros((len(X), lda_emb_len))\n",
    "        for i in range(lda_emb_len):\n",
    "            out[:, i] = lda_que_embs[:, i]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TfIdfTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column = 'whole'):\n",
    "        self.column = column\n",
    "        self.model = TfidfVectorizer(lowercase = False, max_df=0.6, min_df=0.1, analyzer='char_wb', ngram_range=(1,3))\n",
    "    def fit(self, X, y=None):     \n",
    "        self.model = self.model.fit(X[self.column])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        self.model.transform(X[self.column])\n",
    "        return self.model.transform(X[self.column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dG34ANmymZxD"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer(), ['whole']),\n",
    "    (\"structure_transformer\", StructureTransformer(), ['structure']),\n",
    "    #(\"tfidf\", TfIdfTransformer(), ['whole']),\n",
    "    #(\"lda_transformer\", LdaTransformer(), ['whole']),\n",
    "])\n",
    "model = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocess_pipeline),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "      <th>structure</th>\n",
       "      <th>target</th>\n",
       "      <th>whole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>razor user fun us of razored mail</td>\n",
       "      <td>taking razored mail today calling NUMBER NUMBE...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>razor user fun us of razored mail taking razor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spambayes stack pop ate my multipart message</td>\n",
       "      <td>running hammie incoming message noticed multip...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>spambayes stack pop ate my multipart message r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re void a new low on the personal tip</td>\n",
       "      <td>much information saying recall along line put ...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>re void a new low on the personal tip much inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spambayes spambayes package</td>\n",
       "      <td>nasty side effect placing py file package obvi...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>spambayes spambayes package nasty side effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>re a biblical digression</td>\n",
       "      <td>john hall ran across site claimed explain orig...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>re a biblical digression john hall ran across ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        subject                        ...                                                                      whole\n",
       "0             razor user fun us of razored mail                        ...                          razor user fun us of razored mail taking razor...\n",
       "1  spambayes stack pop ate my multipart message                        ...                          spambayes stack pop ate my multipart message r...\n",
       "2         re void a new low on the personal tip                        ...                          re void a new low on the personal tip much inf...\n",
       "3                   spambayes spambayes package                        ...                          spambayes spambayes package nasty side effect ...\n",
       "4                      re a biblical digression                        ...                          re a biblical digression john hall ran across ...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed = data_processed.reset_index(drop=True)\n",
    "random_permutation = np.random.permutation(len(data_processed))\n",
    "data_processed = data_processed.loc[random_permutation]\n",
    "data_processed = data_processed.reset_index(drop=True)\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_processed.drop('target', axis=1).values\n",
    "y = data_processed.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ==============> 1.0\n",
      "1 ==============> 0.9930313588850174\n",
      "2 ==============> 0.9895470383275261\n",
      "3 ==============> 0.9930313588850174\n",
      "4 ==============> 0.9825783972125436\n",
      "5 ==============> 0.9965156794425087\n",
      "6 ==============> 0.9895104895104895\n",
      "7 ==============> 0.9895104895104895\n",
      "8 ==============> 0.9894736842105263\n",
      "9 ==============> 0.9894736842105263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "kfold = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "scores = []\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_test = data_processed.loc[train_index, ['whole', 'structure']], data_processed.loc[test_index, ['whole', 'structure']]\n",
    "    y_train, y_test = data_processed.loc[train_index, ['target']], data_processed.loc[test_index, ['target']]\n",
    "    full_pipeline.fit(X_train, y_train.values.ravel())\n",
    "    predictions = full_pipeline.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test.values.ravel(), predictions))\n",
    "    print(i, '==============>', scores[i])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9912672180194646, 0.00448433447413566)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('wordcount_to_vector', WordCounterToVectorTransformer(column='whole', vocabulary_size=1000), ['whole']), ('structure_transformer', StructureTransf...alty='l2', random_state=42, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline.fit(data_processed.drop('target', axis=1), data_processed.target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from joblib import dump\n",
    "\n",
    "DUMP_PATH = './'\n",
    "d = {'data': data,\n",
    "    'data_processed' : data_processed}\n",
    "\n",
    "with open('dump.pkl', 'wb') as file:\n",
    "    pickle.dump(d, file)\n",
    "    \n",
    "dump(full_pipeline, 'model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb  datasets  dump.pkl  model.joblib\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam_clf.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
