{"cells":[{"metadata":{"id":"7dIrHCzDlZLi","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n# Common imports\nimport numpy as np\nimport os\nimport pandas as pd\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","execution_count":1,"outputs":[]},{"metadata":{"id":"aLQfPuqZlh3s","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import os\nimport tarfile\nfrom six.moves import urllib\n\nDOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\nHAM_URL = DOWNLOAD_ROOT + \"20021010_easy_ham.tar.bz2\"\nHARD_HAM_URL = DOWNLOAD_ROOT + \"20021010_hard_ham.tar.bz2\"\nSPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\nSPAM_PATH = os.path.join(\"datasets\", \"spam\")\n\ndef fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n    if not os.path.isdir(spam_path):\n        os.makedirs(spam_path)\n    for filename, url in ((\"easy_ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL), (\"hard_ham.tar.bz2\", HARD_HAM_URL)):\n        path = os.path.join(spam_path, filename)\n        if not os.path.isfile(path):\n            urllib.request.urlretrieve(url, path)\n        tar_bz2_file = tarfile.open(path)\n        tar_bz2_file.extractall(path=SPAM_PATH)\n        tar_bz2_file.close()","execution_count":2,"outputs":[]},{"metadata":{"id":"uXQbnYI4lkrV","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"fetch_spam_data()","execution_count":3,"outputs":[]},{"metadata":{"id":"2BNzdsZQlnMz","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\nHARD_HAM_DIR = os.path.join(SPAM_PATH, \"hard_ham\")\nSPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\nham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\nhard_ham_filenames = [name for name in sorted(os.listdir(HARD_HAM_DIR)) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]","execution_count":4,"outputs":[]},{"metadata":{"id":"ppMfmYdblpEd","colab_type":"code","outputId":"2252aba5-d24c-4fbb-f74e-ea01c8a3c184","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"len(ham_filenames), len(hard_ham_filenames), len(spam_filenames)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(2551, 250, 500)"},"metadata":{}}]},{"metadata":{"id":"u58xu3tZlrHZ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import email\nimport email.policy\n\ndef load_email(is_spam, filename, spam_path=SPAM_PATH):\n    directory = \"spam\" if is_spam else \"easy_ham\"\n    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)","execution_count":6,"outputs":[]},{"metadata":{"id":"_mVrf1cBls2Q","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def email_to_text(email):\n    for part in email.walk():\n        ctype= part.get_content_type()\n        if not ctype in ('text/plain', 'text/html'):\n            continue \n        try:\n            content= part.get_content()\n        except:\n            content = str(part.get_payload())\n        return content","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\nham_emails = [email for email in ham_emails if TextBlob(email_to_text(email) or 'bonjour').detect_language()=='en']\nspam_emails = [email for email in spam_emails if TextBlob(email_to_text(email) or 'bonjour').detect_language()=='en']","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ham_emails), len(spam_emails)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(2540, 480)"},"metadata":{}}]},{"metadata":{"id":"5qna0WGrlyow","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def get_email_structure(email):\n    if isinstance(email, str):\n        return 'text/plain'\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \", \".join([\n            get_email_structure(sub_email)\n            for sub_email in payload\n        ])\n    else:\n        return email.get_content_type()","execution_count":11,"outputs":[]},{"metadata":{"id":"t_Yo5m-bl0eO","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom itertools import chain\n\nstructures = [get_email_structure(email) for  email in ham_emails]\nCounter(sum([ structure.split(\", \") for structure in structures], [])).most_common()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[('text/plain', 2553),\n ('application/pgp-signature', 73),\n ('text/html', 8),\n ('application/octet-stream', 2),\n ('application/x-pkcs7-signature', 2),\n ('text/enriched', 1),\n ('application/ms-tnef', 1),\n ('video/mng', 1),\n ('text/rfc822-headers', 1),\n ('application/x-java-applet', 1)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"structures = structures + [get_email_structure(email) for  email in spam_emails]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contents = [email_to_text(email) or '' for email in ham_emails]\ncontents = contents + [email_to_text(email) or '' for email in spam_emails]","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subjects = [email['Subject'] for email in ham_emails]\nsubjects = subjects + [email['Subject'] for email in spam_emails]","execution_count":15,"outputs":[]},{"metadata":{"id":"2d4otpe5l-Vw","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.DataFrame({\"subject\" : subjects, \"content\" : contents, \"structure\": structures, \n                     'target' : np.array([0] * len(ham_emails) + [1] * len(spam_emails))})\ndata.drop_duplicates(['subject', 'content'], inplace=True)\ndata.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"                                 subject  ...   target\n0               Re: New Sequences Window  ...        0\n1              [zzzzteana] RE: Alexander  ...        0\n2              [zzzzteana] Moscow bomber  ...        0\n3  [IRR] Klez: The Virus That  Won't Die  ...        0\n4                   Re: Insert signature  ...        0\n\n[5 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>content</th>\n      <th>structure</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Re: New Sequences Window</td>\n      <td>Date:        Wed, 21 Aug 2002 10:54:46 -05...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[zzzzteana] RE: Alexander</td>\n      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[zzzzteana] Moscow bomber</td>\n      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[IRR] Klez: The Virus That  Won't Die</td>\n      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Insert signature</td>\n      <td>On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install urlextract","execution_count":17,"outputs":[{"output_type":"stream","text":"Collecting urlextract\n  Downloading https://files.pythonhosted.org/packages/47/13/d8c5970ba73b0266cb13c6883f9e7cf37b044e52255208ceb32b0d09594a/urlextract-0.10-py3-none-any.whl\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.6/site-packages (from urlextract) (1.4.3)\nCollecting uritools (from urlextract)\n  Downloading https://files.pythonhosted.org/packages/8c/5d/ef3cd3c40b4b97f0cb50cee8e4c5a8a4abc30953e1c7ce7e0d25cb2534c3/uritools-2.2.0-py2.py3-none-any.whl\nRequirement already satisfied: idna in /opt/conda/lib/python3.6/site-packages (from urlextract) (2.6)\nInstalling collected packages: uritools, urlextract\nSuccessfully installed uritools-2.2.0 urlextract-0.10\n\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"id":"_XDL9kEVl_4J","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom html import unescape\nimport urlextract \nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom nltk.stem import WordNetLemmatizer \n\n\n\nclass TextProcessor:\n    \"\"\"\n    Class for carrying all the text pre-processing stuff throughout the project\n    \"\"\"\n\n    def __init__(self):\n        \n        self.stopwords = stopwords.words('english')\n\n        #self.ps = PorterStemmer()  \n        self.lm = WordNetLemmatizer()\n        # stemmer will be used for each unique word once\n        #self.stemmed = dict()\n        self.lemmetized = dict()\n\n        self.url_extractor = urlextract.URLExtract()\n        \n\n    \n    def process(self, text, allow_stopwords = False, use_stemmer = True) :\n        \"\"\"\n        Process the specified text,\n        splitting by non-alphabetic symbols, casting to lower case,\n        removing stopwords, HTML tags and stemming each word\n\n        :param text: text to precess\n        :param allow_stopwords: whether to remove stopwords\n        :return: processed text\n        \"\"\"\n        ret = []\n\n        # split and cast to lower case\n        #text = re.sub(r'<[^>]+>', ' ', str(text))        \n        text = text.lower()\n        text = re.sub(r'[0-9]+(?:\\.[0-9]+){3}', ' URL ', text)\n        urls = list(set(self.url_extractor.find_urls(text)))\n        urls.sort(key=lambda url: len(url), reverse=True)\n        for url in urls:\n            text = text.replace(url, \" URL \")\n            \n        text = re.sub('<head.*?>.*?</head>', '', text, flags=re.M | re.S | re.I)\n        text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n        text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n        text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n        text = unescape(text)\n        text = re.sub(r'\\W+', ' ', text, flags=re.M)\n       \n        \n        text= re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)    \n        \n        for word in text.split():\n            # remove non-alphabetic and stop words\n            if (word.isalpha() and word not in self.stopwords) or allow_stopwords:\n                if use_stemmer:\n                    if word not in self.lemmetized:\n                        self.lemmetized[word] = self.lm.lemmatize(word)\n                    # use stemmed version of word\n                    ret.append(self.lemmetized[word])\n                else: \n                    ret.append(word)\n        return ' '.join(ret)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"                                 subject  ...   target\n0               Re: New Sequences Window  ...        0\n1              [zzzzteana] RE: Alexander  ...        0\n2              [zzzzteana] Moscow bomber  ...        0\n3  [IRR] Klez: The Virus That  Won't Die  ...        0\n4                   Re: Insert signature  ...        0\n\n[5 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>content</th>\n      <th>structure</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Re: New Sequences Window</td>\n      <td>Date:        Wed, 21 Aug 2002 10:54:46 -05...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[zzzzteana] RE: Alexander</td>\n      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[zzzzteana] Moscow bomber</td>\n      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[IRR] Klez: The Virus That  Won't Die</td>\n      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Insert signature</td>\n      <td>On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...</td>\n      <td>text/plain</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp = TextProcessor()\ndata_processed = data.copy()\ndata_processed.content = data_processed.content.apply(lambda x: tp.process(x, allow_stopwords=False, use_stemmer=True))\ndata_processed.subject = data_processed.subject.apply(lambda x : tp.process(x, allow_stopwords = True, use_stemmer=True))\ndata_processed['whole']  = data_processed.subject + ' ' + data_processed.content\ndata_processed.head()","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"                             subject                        ...                                                                      whole\n0             re new sequence window                        ...                          re new sequence window date wed NUMBER aug NUM...\n1             zzzzteana re alexander                        ...                          zzzzteana re alexander martin posted tasso pap...\n2            zzzzteana moscow bomber                        ...                          zzzzteana moscow bomber man threatens explosio...\n3  irr klez the virus that won t die                        ...                          irr klez the virus that won t die klez virus d...\n4                re insert signature                        ...                          re insert signature wed aug NUMBER NUMBER NUMB...\n\n[5 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>content</th>\n      <th>structure</th>\n      <th>target</th>\n      <th>whole</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>re new sequence window</td>\n      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMBE...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>re new sequence window date wed NUMBER aug NUM...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>zzzzteana re alexander</td>\n      <td>martin posted tasso papadopoulos greek sculpto...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>zzzzteana re alexander martin posted tasso pap...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>zzzzteana moscow bomber</td>\n      <td>man threatens explosion moscow thursday august...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>zzzzteana moscow bomber man threatens explosio...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>irr klez the virus that won t die</td>\n      <td>klez virus die already prolific virus ever kle...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>irr klez the virus that won t die klez virus d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>re insert signature</td>\n      <td>wed aug NUMBER NUMBER NUMBER NUMBER ulises pon...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>re insert signature wed aug NUMBER NUMBER NUMB...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"Wc0PnHOPmVuY","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix\n\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000, column = 'whole'):\n        self.vocabulary_size = vocabulary_size\n        self.column = column\n    def fit(self, X, y=None):\n        counter = []\n        for text in X[self.column].values:\n            counter.append(Counter(text.split()))\n        total_count = Counter()\n        for word_count in counter:\n            if isinstance(word_count, list):\n                print(word_count)\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10)\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        counter = []\n        for text in X[self.column].values:\n            counter.append(Counter(text.split()))\n            \n        rows = []\n        cols = []\n        data = []        \n        for row, word_count in enumerate(counter):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)        \n        return pd.DataFrame(columns=['word_UNK'] + ['word_'+column for column in self.vocabulary_], \n                            data=csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1)).toarray())","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nclass StructureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, column = 'structure'):\n        self.column = column\n        \n    def fit(self, X, y=None):     \n        tmp = []\n        for email in X[self.column].apply(lambda x : x.split(', ')).values :\n            for structure in email:\n                tmp.append(structure)        \n        self.structures = list(Counter(tmp).keys())        \n        return self\n    \n    def transform(self, X, y=None):\n        out = np.zeros((len(X), len(self.structures)))\n        for i , structure in enumerate(self.structures):\n            out[:,i] = X[self.column].apply(lambda x : 1 if structure in x.split(', ') else 0).values\n        return out","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom gensim.models.ldamulticore import LdaMulticore\n\nclass LdaTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, dim = 2, column = 'whole'):\n        self.dim = dim\n        self.column = column\n    def fit(self, X, y=None):     \n        lda_tokens = X[self.column].apply(lambda x: x.split())\n        # create Dictionary and train it on text corpus\n        self.lda_dic = Dictionary(lda_tokens)\n        self.lda_dic.filter_extremes(no_below=10, no_above=0.6, keep_n=8000)\n        lda_corpus = [self.lda_dic.doc2bow(doc) for doc in lda_tokens]\n        # create TfidfModel and train it on text corpus\n        self.lda_tfidf = TfidfModel(lda_corpus)\n        lda_corpus = self.lda_tfidf[lda_corpus]\n        # create LDA Model and train it on text corpus\n        self.lda_model = LdaMulticore(\n            lda_corpus, num_topics=self.dim, id2word=self.lda_dic, workers=4,\n            passes=20, chunksize=1000, random_state=0\n        )\n        return self\n    \n    def transform(self, X, y=None):\n        lda_emb_len = len(self.lda_model[[]])\n        lda_corpus = [self.lda_dic.doc2bow(doc) for doc in X[self.column].apply(lambda x: x.split())]\n        lda_corpus = self.lda_tfidf[lda_corpus]\n        lda_que_embs = self.lda_model.inference(lda_corpus)[0]\n        # append lda question embeddings\n        out = np.zeros((len(X), lda_emb_len))\n        for i in range(lda_emb_len):\n            out[:, i] = lda_que_embs[:, i]\n        return out","execution_count":30,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclass TfIdfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, column = 'whole'):\n        self.column = column\n        self.model = TfidfVectorizer(lowercase = False, max_df=0.6, min_df=0.1, analyzer='char_wb', ngram_range=(1,3))\n    def fit(self, X, y=None):     \n        self.model = self.model.fit(X[self.column])\n        return self\n    \n    def transform(self, X, y=None):\n        self.model.transform(X[self.column])\n        return self.model.transform(X[self.column])","execution_count":64,"outputs":[]},{"metadata":{"id":"dG34ANmymZxD","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\npreprocess_pipeline = ColumnTransformer([\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer(), ['whole']),\n    (\"structure_transformer\", StructureTransformer(), ['structure']),\n    #(\"tfidf\", TfIdfTransformer(), ['whole']),\n    #(\"lda_transformer\", LdaTransformer(), ['whole']),\n])\nmodel = LogisticRegression(solver=\"liblinear\", random_state=42)\n\nfull_pipeline = Pipeline([\n    ('preprocessor', preprocess_pipeline),\n    ('model', model)\n])","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed = data_processed.reset_index(drop=True)\nrandom_permutation = np.random.permutation(len(data_processed))\ndata_processed = data_processed.loc[random_permutation]\ndata_processed = data_processed.reset_index(drop=True)\ndata_processed.head()","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"                                             subject                        ...                                                                      whole\n0                           re electric car an edsel                        ...                          re electric car an edsel ah car seen discovery...\n1                               english well for you                        ...                          english well for you hallo found email id dire...\n2     adv lowest life insurance rate available moode                        ...                          adv lowest life insurance rate available moode...\n3  sadev bug NUMBER rpm build put wrong path in t...                        ...                          sadev bug NUMBER rpm build put wrong path in t...\n4  re ilug interesting article on free software l...                        ...                          re ilug interesting article on free software l...\n\n[5 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>content</th>\n      <th>structure</th>\n      <th>target</th>\n      <th>whole</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>re electric car an edsel</td>\n      <td>ah car seen discovery channel url via lurker U...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>re electric car an edsel ah car seen discovery...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>english well for you</td>\n      <td>hallo found email id directoric russian man li...</td>\n      <td>text/plain</td>\n      <td>1</td>\n      <td>english well for you hallo found email id dire...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>adv lowest life insurance rate available moode</td>\n      <td>lowest rate available term life insurance take...</td>\n      <td>text/plain</td>\n      <td>1</td>\n      <td>adv lowest life insurance rate available moode...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sadev bug NUMBER rpm build put wrong path in t...</td>\n      <td>URL URL changed removed added cc spamassassin ...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>sadev bug NUMBER rpm build put wrong path in t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>re ilug interesting article on free software l...</td>\n      <td>david neary said francophones among article su...</td>\n      <td>text/plain</td>\n      <td>0</td>\n      <td>re ilug interesting article on free software l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_processed.drop('target', axis=1).values\ny = data_processed.target.values","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nkfold = StratifiedKFold(n_splits = 10)\n\nscores = []\nfor i, (train_index, test_index) in enumerate(kfold.split(X, y)):\n    X_train, X_test = data_processed.loc[train_index, ['whole', 'structure']], data_processed.loc[test_index, ['whole', 'structure']]\n    y_train, y_test = data_processed.loc[train_index, ['target']], data_processed.loc[test_index, ['target']]\n    full_pipeline.fit(X_train, y_train.values.ravel())\n    predictions = full_pipeline.predict(X_test)\n    scores.append(accuracy_score(y_test.values.ravel(), predictions))\n    print(i, '==============>', scores[i])\n\n\n    ","execution_count":74,"outputs":[{"output_type":"stream","text":"0 ==============> 0.9895470383275261\n1 ==============> 0.9965156794425087\n2 ==============> 0.9860627177700348\n3 ==============> 0.9965156794425087\n4 ==============> 0.9930313588850174\n5 ==============> 0.9860627177700348\n6 ==============> 0.9965034965034965\n7 ==============> 0.986013986013986\n8 ==============> 0.9964912280701754\n9 ==============> 0.9894736842105263\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(scores), np.std(scores)","execution_count":75,"outputs":[{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"(0.9916217586435815, 0.004467691398642306)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"spam_clf.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}